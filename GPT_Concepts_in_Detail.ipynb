{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# read the file\n",
        "with open(\"scripts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "ikGOG3k67fLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of dataset in characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3YJLb1795e",
        "outputId": "374ed3a1-7580-4652-f205-e996df171844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 4939063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCknE_U-8G86",
        "outputId": "2a2fa3a3-dfb8-472b-e9e7-93d006ea24c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kids, breakfast! Kids? Phil, would you get them? Yeah, just a sec.\n",
            " That is so - Kids, get down here! Why are you guys yelling at us? When we're way upstairs, just text me.\n",
            " All right, that's not gonna happen, and, wow, you're not wearing that outfit.\n",
            " What's wrong with it? - Honey, do you have anything to say to your daughter about her skirt? Sorry? Oh yeah, that looks really cute, sweetheart! Thanks! - No, it's way too short, people know you're a girl you don't need to prove it to them.\n",
            " Luke got his head stuck in the banister again.\n",
            " I got it.\n",
            " Where's the baby oil? - It's on our bedside tab I don't know, find it.\n",
            " Come on! I was out of control growing up.\n",
            " There, you know, I said it.\n",
            " I just don't want my kids to make the same bad mistakes I made.\n",
            " If Hayley never wakes up on a beach in Florida, half-naked I've done my job.\n",
            " OUR job.\n",
            " - Right I've done our job.\n",
            " That was a penalty! Gloria, they're 0 and 6, let's take it down a notch.\n",
            " We're very different.\n",
            " He's from the city.\n",
            " He \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the unique characters present in the text in a list\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmcd8KSy8Pb_",
        "outputId": "494debf8-1c66-4df4-aa88-5fb37d621701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "\u001f !\"#$%&'()*,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz ¡¢£¤¨©ª­®±³´º¿ÂÃáâéíñóú\n",
            "117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers in order to prepare for tokenization\n",
        "string_to_integers = {char:i for i,char in enumerate(chars)}\n",
        "integers_to_string = {i:char for i,char in enumerate(chars)}\n",
        "encode = lambda s: [string_to_integers[c] for c in s]   # encoder : takes in string, outputs a list of integers\n",
        "decode = lambda l: ''.join([integers_to_string[i] for i in l])   # decoder : takes in list of integers, outputs a string\n",
        "\n",
        "print(encode(\"This is an encoded message\"))\n",
        "print(decode(encode(\"This is an encoded message\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_JwsPVp9Abh",
        "outputId": "d898579f-c43d-4f49-87db-b03695fde18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51, 68, 69, 79, 3, 69, 79, 3, 61, 74, 3, 65, 74, 63, 75, 64, 65, 64, 3, 73, 65, 79, 79, 61, 67, 65]\n",
            "This is an encoded message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We now encode the entire text and store it into a torch.Tensor\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])   # the 1000 characters will look like this after getting tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I73H2BI5_F2Y",
        "outputId": "5d979b9b-9f41-4834-8c80-5007e9fd9bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4939063]) torch.int64\n",
            "tensor([42, 69, 64, 79, 14,  3, 62, 78, 65, 61, 71, 66, 61, 79, 80,  4,  3, 42,\n",
            "        69, 64, 79, 30,  3, 47, 68, 69, 72, 14,  3, 83, 75, 81, 72, 64,  3, 85,\n",
            "        75, 81,  3, 67, 65, 80,  3, 80, 68, 65, 73, 30,  3, 56, 65, 61, 68, 14,\n",
            "         3, 70, 81, 79, 80,  3, 61,  3, 79, 65, 63, 16,  1,  3, 51, 68, 61, 80,\n",
            "         3, 69, 79,  3, 79, 75,  3, 15,  3, 42, 69, 64, 79, 14,  3, 67, 65, 80,\n",
            "         3, 64, 75, 83, 74,  3, 68, 65, 78, 65,  4,  3, 54, 68, 85,  3, 61, 78,\n",
            "        65,  3, 85, 75, 81,  3, 67, 81, 85, 79,  3, 85, 65, 72, 72, 69, 74, 67,\n",
            "         3, 61, 80,  3, 81, 79, 30,  3, 54, 68, 65, 74,  3, 83, 65, 10, 78, 65,\n",
            "         3, 83, 61, 85,  3, 81, 76, 79, 80, 61, 69, 78, 79, 14,  3, 70, 81, 79,\n",
            "        80,  3, 80, 65, 84, 80,  3, 73, 65, 16,  1,  3, 32, 72, 72,  3, 78, 69,\n",
            "        67, 68, 80, 14,  3, 80, 68, 61, 80, 10, 79,  3, 74, 75, 80,  3, 67, 75,\n",
            "        74, 74, 61,  3, 68, 61, 76, 76, 65, 74, 14,  3, 61, 74, 64, 14,  3, 83,\n",
            "        75, 83, 14,  3, 85, 75, 81, 10, 78, 65,  3, 74, 75, 80,  3, 83, 65, 61,\n",
            "        78, 69, 74, 67,  3, 80, 68, 61, 80,  3, 75, 81, 80, 66, 69, 80, 16,  1,\n",
            "         3, 54, 68, 61, 80, 10, 79,  3, 83, 78, 75, 74, 67,  3, 83, 69, 80, 68,\n",
            "         3, 69, 80, 30,  3, 15,  3, 39, 75, 74, 65, 85, 14,  3, 64, 75,  3, 85,\n",
            "        75, 81,  3, 68, 61, 82, 65,  3, 61, 74, 85, 80, 68, 69, 74, 67,  3, 80,\n",
            "        75,  3, 79, 61, 85,  3, 80, 75,  3, 85, 75, 81, 78,  3, 64, 61, 81, 67,\n",
            "        68, 80, 65, 78,  3, 61, 62, 75, 81, 80,  3, 68, 65, 78,  3, 79, 71, 69,\n",
            "        78, 80, 30,  3, 50, 75, 78, 78, 85, 30,  3, 46, 68,  3, 85, 65, 61, 68,\n",
            "        14,  3, 80, 68, 61, 80,  3, 72, 75, 75, 71, 79,  3, 78, 65, 61, 72, 72,\n",
            "        85,  3, 63, 81, 80, 65, 14,  3, 79, 83, 65, 65, 80, 68, 65, 61, 78, 80,\n",
            "         4,  3, 51, 68, 61, 74, 71, 79,  4,  3, 15,  3, 45, 75, 14,  3, 69, 80,\n",
            "        10, 79,  3, 83, 61, 85,  3, 80, 75, 75,  3, 79, 68, 75, 78, 80, 14,  3,\n",
            "        76, 65, 75, 76, 72, 65,  3, 71, 74, 75, 83,  3, 85, 75, 81, 10, 78, 65,\n",
            "         3, 61,  3, 67, 69, 78, 72,  3, 85, 75, 81,  3, 64, 75, 74, 10, 80,  3,\n",
            "        74, 65, 65, 64,  3, 80, 75,  3, 76, 78, 75, 82, 65,  3, 69, 80,  3, 80,\n",
            "        75,  3, 80, 68, 65, 73, 16,  1,  3, 43, 81, 71, 65,  3, 67, 75, 80,  3,\n",
            "        68, 69, 79,  3, 68, 65, 61, 64,  3, 79, 80, 81, 63, 71,  3, 69, 74,  3,\n",
            "        80, 68, 65,  3, 62, 61, 74, 69, 79, 80, 65, 78,  3, 61, 67, 61, 69, 74,\n",
            "        16,  1,  3, 40,  3, 67, 75, 80,  3, 69, 80, 16,  1,  3, 54, 68, 65, 78,\n",
            "        65, 10, 79,  3, 80, 68, 65,  3, 62, 61, 62, 85,  3, 75, 69, 72, 30,  3,\n",
            "        15,  3, 40, 80, 10, 79,  3, 75, 74,  3, 75, 81, 78,  3, 62, 65, 64, 79,\n",
            "        69, 64, 65,  3, 80, 61, 62,  3, 40,  3, 64, 75, 74, 10, 80,  3, 71, 74,\n",
            "        75, 83, 14,  3, 66, 69, 74, 64,  3, 69, 80, 16,  1,  3, 34, 75, 73, 65,\n",
            "         3, 75, 74,  4,  3, 40,  3, 83, 61, 79,  3, 75, 81, 80,  3, 75, 66,  3,\n",
            "        63, 75, 74, 80, 78, 75, 72,  3, 67, 78, 75, 83, 69, 74, 67,  3, 81, 76,\n",
            "        16,  1,  3, 51, 68, 65, 78, 65, 14,  3, 85, 75, 81,  3, 71, 74, 75, 83,\n",
            "        14,  3, 40,  3, 79, 61, 69, 64,  3, 69, 80, 16,  1,  3, 40,  3, 70, 81,\n",
            "        79, 80,  3, 64, 75, 74, 10, 80,  3, 83, 61, 74, 80,  3, 73, 85,  3, 71,\n",
            "        69, 64, 79,  3, 80, 75,  3, 73, 61, 71, 65,  3, 80, 68, 65,  3, 79, 61,\n",
            "        73, 65,  3, 62, 61, 64,  3, 73, 69, 79, 80, 61, 71, 65, 79,  3, 40,  3,\n",
            "        73, 61, 64, 65, 16,  1,  3, 40, 66,  3, 39, 61, 85, 72, 65, 85,  3, 74,\n",
            "        65, 82, 65, 78,  3, 83, 61, 71, 65, 79,  3, 81, 76,  3, 75, 74,  3, 61,\n",
            "         3, 62, 65, 61, 63, 68,  3, 69, 74,  3, 37, 72, 75, 78, 69, 64, 61, 14,\n",
            "         3, 68, 61, 72, 66, 15, 74, 61, 71, 65, 64,  3, 40, 10, 82, 65,  3, 64,\n",
            "        75, 74, 65,  3, 73, 85,  3, 70, 75, 62, 16,  1,  3, 46, 52, 49,  3, 70,\n",
            "        75, 62, 16,  1,  3, 15,  3, 49, 69, 67, 68, 80,  3, 40, 10, 82, 65,  3,\n",
            "        64, 75, 74, 65,  3, 75, 81, 78,  3, 70, 75, 62, 16,  1,  3, 51, 68, 61,\n",
            "        80,  3, 83, 61, 79,  3, 61,  3, 76, 65, 74, 61, 72, 80, 85,  4,  3, 38,\n",
            "        72, 75, 78, 69, 61, 14,  3, 80, 68, 65, 85, 10, 78, 65,  3, 18,  3, 61,\n",
            "        74, 64,  3, 24, 14,  3, 72, 65, 80, 10, 79,  3, 80, 61, 71, 65,  3, 69,\n",
            "        80,  3, 64, 75, 83, 74,  3, 61,  3, 74, 75, 80, 63, 68, 16,  1,  3, 54,\n",
            "        65, 10, 78, 65,  3, 82, 65, 78, 85,  3, 64, 69, 66, 66, 65, 78, 65, 74,\n",
            "        80, 16,  1,  3, 39, 65, 10, 79,  3, 66, 78, 75, 73,  3, 80, 68, 65,  3,\n",
            "        63, 69, 80, 85, 16,  1,  3, 39, 65,  3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "n = int(0.9*len(data))   # first 90% will be train, rest will be validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "6xPm2yHD_zlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the context length (block size) for training\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohQGoLwxACHW",
        "outputId": "ced50643-a1a4-4eed-9a42-3960edf91385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([42, 69, 64, 79, 14,  3, 62, 78, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling out how the process of training will work\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9apBdbgaA1xc",
        "outputId": "ed6da0f4-d3cd-44ca-d26d-f2700c5b4a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([42]), the target is: 69\n",
            "When input is tensor([42, 69]), the target is: 64\n",
            "When input is tensor([42, 69, 64]), the target is: 79\n",
            "When input is tensor([42, 69, 64, 79]), the target is: 14\n",
            "When input is tensor([42, 69, 64, 79, 14]), the target is: 3\n",
            "When input is tensor([42, 69, 64, 79, 14,  3]), the target is: 62\n",
            "When input is tensor([42, 69, 64, 79, 14,  3, 62]), the target is: 78\n",
            "When input is tensor([42, 69, 64, 79, 14,  3, 62, 78]), the target is: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will now generate batches of data, each consisting of block_size of data in it, for parallel processing.\n",
        "\n",
        "torch.manual_seed(1337)   # ensures reproducibility by setting the seed for generating random numbers in PyTorch.\n",
        "batch_size = 4   # how many independent sequences will we process in parallel?\n",
        "block_size = 8   # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "for b in range(batch_size):   # batch dimension\n",
        "  for t in range(block_size):   # time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"When input is {context.tolist()}, the target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyD20fUyBSsJ",
        "outputId": "173b9ae5-a3e5-4722-8de4-b57447e17310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[75, 81,  3, 71, 74, 75, 83, 16],\n",
            "        [72, 72, 85,  3, 67, 72, 61, 64],\n",
            "        [16,  1,  3, 44, 73, 15, 68, 73],\n",
            "        [80,  3, 69, 80,  3, 62, 65, 68]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[81,  3, 71, 74, 75, 83, 16,  1],\n",
            "        [72, 85,  3, 67, 72, 61, 64,  3],\n",
            "        [ 1,  3, 44, 73, 15, 68, 73, 73],\n",
            "        [ 3, 69, 80,  3, 62, 65, 68, 69]])\n",
            "-----\n",
            "When input is [75], the target is: 81\n",
            "When input is [75, 81], the target is: 3\n",
            "When input is [75, 81, 3], the target is: 71\n",
            "When input is [75, 81, 3, 71], the target is: 74\n",
            "When input is [75, 81, 3, 71, 74], the target is: 75\n",
            "When input is [75, 81, 3, 71, 74, 75], the target is: 83\n",
            "When input is [75, 81, 3, 71, 74, 75, 83], the target is: 16\n",
            "When input is [75, 81, 3, 71, 74, 75, 83, 16], the target is: 1\n",
            "When input is [72], the target is: 72\n",
            "When input is [72, 72], the target is: 85\n",
            "When input is [72, 72, 85], the target is: 3\n",
            "When input is [72, 72, 85, 3], the target is: 67\n",
            "When input is [72, 72, 85, 3, 67], the target is: 72\n",
            "When input is [72, 72, 85, 3, 67, 72], the target is: 61\n",
            "When input is [72, 72, 85, 3, 67, 72, 61], the target is: 64\n",
            "When input is [72, 72, 85, 3, 67, 72, 61, 64], the target is: 3\n",
            "When input is [16], the target is: 1\n",
            "When input is [16, 1], the target is: 3\n",
            "When input is [16, 1, 3], the target is: 44\n",
            "When input is [16, 1, 3, 44], the target is: 73\n",
            "When input is [16, 1, 3, 44, 73], the target is: 15\n",
            "When input is [16, 1, 3, 44, 73, 15], the target is: 68\n",
            "When input is [16, 1, 3, 44, 73, 15, 68], the target is: 73\n",
            "When input is [16, 1, 3, 44, 73, 15, 68, 73], the target is: 73\n",
            "When input is [80], the target is: 3\n",
            "When input is [80, 3], the target is: 69\n",
            "When input is [80, 3, 69], the target is: 80\n",
            "When input is [80, 3, 69, 80], the target is: 3\n",
            "When input is [80, 3, 69, 80, 3], the target is: 62\n",
            "When input is [80, 3, 69, 80, 3, 62], the target is: 65\n",
            "When input is [80, 3, 69, 80, 3, 62, 65], the target is: 68\n",
            "When input is [80, 3, 69, 80, 3, 62, 65, 68], the target is: 69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)   # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsUCIytQOcLx",
        "outputId": "06f19a8e-9fcc-4379-aa9c-ece102248801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[75, 81,  3, 71, 74, 75, 83, 16],\n",
            "        [72, 72, 85,  3, 67, 72, 61, 64],\n",
            "        [16,  1,  3, 44, 73, 15, 68, 73],\n",
            "        [80,  3, 69, 80,  3, 62, 65, 68]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>IMPLEMENTING A SIMPLE BIGRAM NN MODEL</h2>"
      ],
      "metadata": {
        "id": "YNUriwCwUOED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx and targets are both (B,T) tensor of integers\n",
        "    # logits are basically predictions\n",
        "    logits = self.token_embedding_table(idx)   # (B,T,C) - Batch (4) by Time (8) by Channel (vocab_size, i.e, 65) tensor\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :]   # becomes (B,C)\n",
        "      # apply softmax to get probabilies\n",
        "      probs = F.softmax(logits, dim=-1)   # (B,C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)   # (B,1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1)   # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg6_QIeEUIhA",
        "outputId": "53290b35-3c0f-4679-9665-539f12de5361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 117])\n",
            "tensor(5.4761, grad_fn=<NllLossBackward0>)\n",
            "\tWÃVm%Kh]yfú-ó1\n",
            "qá?]M#)fLBM\"@x(!ª(%8B±OeL¤)ú_(£ 4®0ÂA4q.-k&u_?ú\n",
            "!O£B67T\";-AxWIHg©­sa¨F2L_é;t[Fíj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>TRAINING THE MODEL</h2>"
      ],
      "metadata": {
        "id": "-22q-0o3bss1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch Optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "oFNdvQ65bvbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training loop\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch(\"train\")\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvwZRHgocB_x",
        "outputId": "1f2adc4f-d68d-433f-da8e-a47c490d46d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5171384811401367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing how well the generation is, after optimizing the loss\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=600)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-LULMXddHDs",
        "outputId": "38515e2c-bef5-419d-90ff-76bb4ed0e35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t%I- st?  Bule IfYe Al thinure wougo N;UI as en âSth, a- Youlld thtowars.]\n",
            " h!? - as y ly germinke.\n",
            " j¡1 Bu   maybyoifrbt Itofan t ferk! e Yeraliresmevinanthicoliond le'dowhe it I byotis The Man g yourellyoung wameriak.\n",
            " brcath indot t? ganªâªjut  e r.\n",
            "\"¿2juacod.\n",
            " coukecn'ththakitho ind  t? Fin's,moyoole l, No nempramigowocutgis g Norwhewine's Yen'Sut g.\n",
            " tors mes.\n",
            " \"oweve ma ay're f I Muatom.\n",
            " RSre chane t - hameay, theanaveronent't! Theann foa s, tis I Jak my Ittm s roweasoom?  rindomicke.\n",
            " cu'st inus.\n",
            " Gondou hed.\n",
            " t b-yl! Jalyow, t Whom wn wemomay.\n",
            "\" doug Man's, bothe.]\n",
            " thinn Is [Lupsh, wé\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Introducing Self Attention</h2>\n",
        "\n",
        "The following section discusses different approaches of performing weighted aggregation of tokens, for the purpose of Self-Attention, i.e, the tokens can see each other in the past context (not the future), and decide which one influences more and which one less on itself."
      ],
      "metadata": {
        "id": "26-MdUH_ptV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a dummy example :\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2   # batch, time, channel\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLtGW3KypyJQ",
        "outputId": "5ad37893-73e4-4d4e-d8af-1b782455cb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>APPROACH 1"
      ],
      "metadata": {
        "id": "zl1oYwjwCG6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 1 - Simple Average of Past Tokens (Bag-Of-Words)\n",
        "\n",
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))   # \"bow\" stands for Bag-Of-Words\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]   # (t,C)\n",
        "    xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "Y3Ra-dw2p_S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>APPROACH 2</h3>"
      ],
      "metadata": {
        "id": "jHoy1AbFCOuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An efficient way to make the tokens communicate with each other instead of simply averaging the previous and current timestamps\n",
        "# for the channels is to use Matrix Multiplication :\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print(\"a :\")\n",
        "print(a)\n",
        "print(\"---\")\n",
        "print(\"b :\")\n",
        "print(b)\n",
        "print(\"---\")\n",
        "print(\"c :\")\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou-N24--qr1O",
        "outputId": "6ebccd08-02f2-4252-f2e9-0cceea961520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a :\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---\n",
            "b :\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---\n",
            "c :\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 2 - Using Matrix Multiplication to better aggregate the weights of tokens\n",
        "\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x   # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE4FPBw9Bg7i",
        "outputId": "d337f49d-2e9f-4a8d-a2cc-b9581fe93013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>APPROACH 3</h3>"
      ],
      "metadata": {
        "id": "-P1ycAYfCWtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 3 - Another approach is to use Softmax :\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVepU2S0raIR",
        "outputId": "dfc27f28-d00f-4373-d4e1-c0861fa0d05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>APPROACH 4</h3>"
      ],
      "metadata": {
        "id": "VK5sW6OiCc9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 4 - The crux of Self-Attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32   # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# Assume a single Head performing self-attention :\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)   # \"key\" of a token represents the information it holds\n",
        "query = nn.Linear(C, head_size, bias=False)   # \"query\" of a token represents the information it requires\n",
        "value = nn.Linear(C, head_size, bias=False)   # \"value\" holds the aggregation of past tokens upto a specified token\n",
        "k = key(x)   # (B,T,16)\n",
        "q = query(x)   # (B,T,16)\n",
        "wei = q @ k.transpose(-2, -1)   # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw1knnj8BooM",
        "outputId": "4e749785-c304-4053-9e42-1f6a67998696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMOw0XMICia8",
        "outputId": "ae64a8c9-c09a-45b6-9a76-a28dc7904b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Implementing Layer Normalization (LayerNorm)</h3>"
      ],
      "metadata": {
        "id": "XfqtrJaYSYUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True)   # batch mean\n",
        "    xvar = x.var(1, keepdim=True)   # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)   # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100)   # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIzm5ELjSWtZ",
        "outputId": "5a8d2c69-4e26-4687-bb6a-c2eb94a20272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std()   # mean, std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQlTrtmQS4rE",
        "outputId": "dbd33fd2-79fd-4dca-aa2f-6ace71e480e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std()   # mean, std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GNplGn-TB-P",
        "outputId": "abb4583a-da67-43a5-9811-32264733f709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}